apiVersion: batch/v1
kind: CronJob
metadata:
  name: pg-backup-cron
  namespace: project
spec:
  schedule: "0 0 * * *"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: pg-backup
              # Using PostgreSQL 17 image which has matching client version
              image: postgres:17-alpine
              command:
                - /bin/sh
                - -c
                - |
                  set -e

                  # Install Google Cloud SDK
                  apk add --no-cache curl python3 py3-pip bash coreutils
                  curl -sSL https://sdk.cloud.google.com | bash
                  export PATH=$PATH:/root/google-cloud-sdk/bin

                  # Activate service account
                  gcloud auth activate-service-account --key-file=/secrets/gcs/key.json

                  # Set the project (replace with your actual project ID)
                  gcloud config set project $GCP_PROJECT_ID

                  # Test database connection first
                  echo "Testing database connection..."
                  if ! pg_isready -h "$DB_HOST" -U "$DB_USER" -d "$DB_NAME"; then
                    echo "ERROR: Database is not ready"
                    exit 1
                  fi

                  # Check if database exists
                  echo "Checking if database exists..."
                  DB_EXISTS=$(psql -h "$DB_HOST" -U "$DB_USER" -d postgres -t -c "SELECT 1 FROM pg_database WHERE datname='$DB_NAME';" 2>/dev/null || echo "")
                  if [ -z "$DB_EXISTS" ]; then
                    echo "ERROR: Database '$DB_NAME' does not exist"
                    echo "Available databases:"
                    psql -h "$DB_HOST" -U "$DB_USER" -d postgres -c "\l"
                    exit 1
                  fi

                  # Check if database has any tables
                  echo "Checking database contents..."
                  psql -h "$DB_HOST" -U "$DB_USER" -d "$DB_NAME" -c "\dt" || echo "No tables found"

                  # Get table count
                  TABLE_COUNT=$(psql -h "$DB_HOST" -U "$DB_USER" -d "$DB_NAME" -t -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';" 2>/dev/null | xargs)
                  echo "Number of tables in database: $TABLE_COUNT"

                  # Warn if database is empty but continue with backup
                  if [ "$TABLE_COUNT" = "0" ]; then
                    echo "WARNING: Database appears to be empty (no tables found)"
                    echo "Backup will still be created but will contain no data"
                  fi

                  # Create backup with timestamp
                  BACKUP_FILE="backup-$(date +%Y-%m-%d-%H-%M-%S).sql.gz"
                  echo "Creating backup: $BACKUP_FILE"

                  # Create backup directory
                  mkdir -p /tmp/backups

                  # Perform backup with more verbose output
                  echo "Starting pg_dump..."
                  pg_dump -h "$DB_HOST" -U "$DB_USER" -d "$DB_NAME" --verbose --no-password | gzip > /tmp/backups/$BACKUP_FILE

                  # Check backup size
                  BACKUP_SIZE=$(stat -c%s /tmp/backups/$BACKUP_FILE)
                  echo "Backup file size: $BACKUP_SIZE bytes"

                  # Show a sample of the backup content (first 50 lines)
                  echo "Backup content preview:"
                  zcat /tmp/backups/$BACKUP_FILE | head -n 50

                  # Upload to GCS using gsutil
                  echo "Uploading to GCS..."
                  gsutil cp /tmp/backups/$BACKUP_FILE gs://$GCS_BUCKET/

                  # Verify upload
                  gsutil ls gs://$GCS_BUCKET/$BACKUP_FILE

                  # Cleanup old backups (keep last 30 days) - Fixed date command for Alpine
                  echo "Cleaning up old backups..."
                  CUTOFF_DATE=$(date -d '30 days ago' +%Y-%m-%d 2>/dev/null || date -d '@'$(($(date +%s) - 30*24*3600)) +%Y-%m-%d)

                  gsutil ls gs://$GCS_BUCKET/backup-*.sql.gz | while read file; do
                    # Extract date from filename
                    filename=$(basename "$file")
                    if [[ $filename =~ backup-([0-9]{4}-[0-9]{2}-[0-9]{2}) ]]; then
                      file_date="${BASH_REMATCH[1]}"
                      if [[ "$file_date" < "$CUTOFF_DATE" ]]; then
                        echo "Deleting old backup: $file"
                        gsutil rm "$file"
                      fi
                    fi
                  done

                  echo "Backup completed successfully at $(date)"
              env:
                - name: PGPASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secret
                      key: PGPASSWORD
                - name: DB_HOST
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secret
                      key: POSTGRES_HOST
                - name: DB_USER
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secret
                      key: POSTGRES_USER
                - name: DB_NAME
                  value: "postgres" #name of my database
                - name: GCS_BUCKET
                  value: "my-unique-postgres-backup-bucket"
                - name: GCP_PROJECT_ID
                  value: "master-coder-465618-q6"
              volumeMounts:
                - name: gcs-secret
                  mountPath: "/secrets/gcs"
                  readOnly: true
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"
          volumes:
            - name: gcs-secret
              secret:
                secretName: gcs-secret
          restartPolicy: OnFailure
