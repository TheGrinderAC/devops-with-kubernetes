apiVersion: batch/v1
kind: CronJob
metadata:
  name: pg-backup-cron
spec:
  schedule: "0 0 * * *" # daily at midnight
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: pg-backup
              # Using Google Cloud SDK Alpine image which includes gsutil and python >
              image: google/cloud-sdk:alpine
              command:
                - /bin/sh
                - -c
                - |
                  set -e

                  # Install PostgreSQL client tools
                  apk add --no-cache postgresql-client

                  echo "Starting backup process..."
                  echo "Database host: $DB_HOST"
                  echo "Database user: $DB_USER"
                  echo "Database name: $DB_NAME"
                  echo "GCS Bucket: $GCS_BUCKET"

                  # Test database connection first
                  echo "Testing database connection..."
                  if ! pg_isready -h "$DB_HOST" -U "$DB_USER" -d "$DB_NAME" -t 30; then
                    echo "ERROR: Cannot connect to database"
                    exit 1
                  fi

                  # Test if database exists using direct SQL query (version-independent)
                  echo "Checking if database exists..."
                  DB_EXISTS=$(psql -h "$DB_HOST" -U "$DB_USER" -d postgres -tAc "SELECT 1 FROM pg_database WHERE datname='$DB_NAME'" 2>/dev/null || echo "0")

                  if [ "$DB_EXISTS" != "1" ]; then
                    echo "ERROR: Database '$DB_NAME' does not exist"
                    echo "Available databases:"
                    psql -h "$DB_HOST" -U "$DB_USER" -d postgres -tAc "SELECT datname FROM pg_database WHERE datistemplate = false" 2>/dev/null || echo "Could not list databases"
                    exit 1
                  fi
                  echo "Database '$DB_NAME' exists"

                  # Test GCS authentication
                  echo "Testing GCS authentication..."
                  if ! gsutil ls "gs://$GCS_BUCKET/" > /dev/null 2>&1; then
                    echo "ERROR: Cannot access GCS bucket gs://$GCS_BUCKET/"
                    echo "Please check GCS credentials and bucket permissions"
                    exit 1
                  fi

                  # Create backup filename with timestamp
                  BACKUP_FILE="backup-$(date +%Y-%m-%d-%H-%M-%S).sql.gz"
                  GCS_PATH="gs://$GCS_BUCKET/$BACKUP_FILE"

                  echo "Starting backup of $DB_NAME database..."

                  # Create backup with better error handling
                  if ! pg_dump -h "$DB_HOST" -U "$DB_USER" -d "$DB_NAME" --verbose | gzip > /tmp/backup.sql.gz; then
                    echo "ERROR: pg_dump failed"
                    exit 1
                  fi

                  # Verify backup file exists and has content
                  if [ ! -s /tmp/backup.sql.gz ]; then
                    echo "ERROR: Backup file is empty or missing"
                    exit 1
                  fi

                  BACKUP_SIZE=$(du -h /tmp/backup.sql.gz | cut -f1)
                  echo "Backup created successfully, size: $BACKUP_SIZE"

                  # Upload to GCS
                  echo "Uploading backup to GCS..."
                  if ! gsutil cp /tmp/backup.sql.gz "$GCS_PATH"; then
                    echo "ERROR: Failed to upload backup to GCS"
                    exit 1
                  fi

                  # Verify upload
                  if gsutil ls "$GCS_PATH" > /dev/null 2>&1; then
                    echo "Backup uploaded successfully to $GCS_PATH"
                  else
                    echo "ERROR: Failed to verify backup upload"
                    exit 1
                  fi

                  # Clean up local file
                  rm -f /tmp/backup.sql.gz

                  # Clean up old backups (keep last 30 days)
                  echo "Cleaning up backups older than 30 days..."
                  CUTOFF_DATE=$(date -d '30 days ago' +%s)
                  gsutil ls "gs://$GCS_BUCKET/backup-*.sql.gz" 2>/dev/null | \
                    while read file; do
                      if [ -n "$file" ]; then
                        # Extract date from filename (backup-YYYY-MM-DD-HH-MM-SS.sql.gz)
                        file_date=$(echo "$file" | sed 's/.*backup-\([0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}\)-.*/\1/')
                        if [ "$file_date" != "" ] && [ "$file_date" != "$file" ]; then
                          # Convert to timestamp for comparison
                          file_timestamp=$(date -d "$file_date" +%s 2>/dev/null || echo "0")
                          
                          if [ $file_timestamp -lt $CUTOFF_DATE ]; then
                            days_old=$(( ($(date +%s) - file_timestamp) / 86400 ))
                            echo "Deleting old backup: $file (${days_old} days old)"
                            gsutil rm "$file"
                          fi
                        fi
                      fi
                    done

                  echo "Backup process completed successfully"
              env:
                - name: PGPASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secret
                      key: PGPASSWORD
                - name: GOOGLE_APPLICATION_CREDENTIALS
                  value: /secrets/gcs/key.json
                # Database configuration from secret
                - name: DB_HOST
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secret
                      key: POSTGRES_HOST
                - name: DB_USER
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secret
                      key: POSTGRES_USER
                - name: DB_NAME
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secret
                      key: POSTGRES_DB
                - name: GCS_BUCKET
                  value: "my-unique-postgres-backup-bucket"
              volumeMounts:
                - name: gcs-secret
                  mountPath: "/secrets/gcs"
                  readOnly: true
              resources:
                requests:
                  memory: "128Mi"
                  cpu: "100m"
          volumes:
            - name: gcs-secret
              secret:
                secretName: gcs-secret
          restartPolicy: OnFailure
