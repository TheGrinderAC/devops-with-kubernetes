# This CronJob makes a backup of the todo database every 24 hours and saves it to Google Cloud Storage.
# It also includes backup verification and cleanup of old backups (keeps last 30 days).

# Created the Kubernetes secret:
#    kubectl create secret generic gcs-secret --from-file=key.json

apiVersion: batch/v1
kind: CronJob
metadata:
  name: pg-backup-cron
spec:
  schedule: "0 0 * * *" # daily at midnight
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: pg-backup
              image: prodrigestivill/postgres-gcs-backup
              command:
                - /bin/sh
                - -c
                - |
                  set -e

                  # Create backup filename with timestamp
                  BACKUP_FILE="backup-$(date +%Y-%m-%d-%H-%M-%S).sql.gz"
                  GCS_PATH="gs://postgress_backup_sql/$BACKUP_FILE"

                  echo "Starting backup of todo-db database..."

                  # Create backup
                  pg_dump -h postgres -U postgres todo-db | gzip > /tmp/backup.sql.gz

                  # Verify backup file exists and has content
                  if [ ! -s /tmp/backup.sql.gz ]; then
                    echo "ERROR: Backup file is empty or missing"
                    exit 1
                  fi

                  echo "Backup created successfully, size: $(du -h /tmp/backup.sql.gz | cut -f1)"

                  # Upload to GCS
                  gsutil cp /tmp/backup.sql.gz "$GCS_PATH"

                  # Verify upload
                  if gsutil ls "$GCS_PATH" > /dev/null 2>&1; then
                    echo "Backup uploaded successfully to $GCS_PATH"
                  else
                    echo "ERROR: Failed to verify backup upload"
                    exit 1
                  fi

                  # Clean up local file
                  rm -f /tmp/backup.sql.gz

                  # Clean up old backups (keep last 30 days)
                  echo "Cleaning up backups older than 30 days..."
                  gsutil ls gs://postgress_backup_sql/backup-*.sql.gz | \
                    while read file; do
                      # Extract date from filename (backup-YYYY-MM-DD-HH-MM-SS.sql.gz)
                      file_date=$(echo "$file" | sed 's/.*backup-\([0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}\)-.*/\1/')
                      if [ "$file_date" != "" ]; then
                        # Convert to timestamp for comparison
                        file_timestamp=$(date -d "$file_date" +%s 2>/dev/null || echo "0")
                        current_timestamp=$(date +%s)
                        days_old=$(( (current_timestamp - file_timestamp) / 86400 ))
                        
                        if [ $days_old -gt 30 ]; then
                          echo "Deleting old backup: $file (${days_old} days old)"
                          gsutil rm "$file"
                        fi
                      fi
                    done

                  echo "Backup process completed successfully"
              env:
                - name: PGPASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secret
                      key: PGPASSWORD
                - name: GOOGLE_APPLICATION_CREDENTIALS
                  value: /secrets/gcs/key.json
              volumeMounts:
                - name: gcs-secret
                  mountPath: "/secrets/gcs"
                  readOnly: true
          volumes:
            - name: gcs-secret
              secret:
                secretName: gcs-secret
          restartPolicy: OnFailure
